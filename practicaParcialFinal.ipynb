{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "try:\n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "    \n",
    "type(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 1), (6, 1), (8, 1), (34, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tupla(valor):\n",
    "    \n",
    "    if (x > 5):\n",
    "        \n",
    "        return (tuple(x,1))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return (tuple(x,0))\n",
    "##########################################################################\n",
    "lista = [3,5,8,2,5,6,8,0,1,34]\n",
    "\n",
    "rdd = sc.parallelize(lista,3)\n",
    "\n",
    "rdd.map(lambda x: (x,1) if x > 5 else (x,0))\\\n",
    "   .filter(lambda x: x[1] == 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('santa fe', 3)\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "#Resolucion del examen recuperatortio\n",
    "#Ejercicio 1\n",
    "\n",
    "censo = [('bs.as', 'cf', 20000), \n",
    "         ('santa fe', 'rosario', 30000),\n",
    "         ('bs.as', 'mar del plata', 10000),\n",
    "         ('bs.as', 'la matanza', 40000),\n",
    "         ('jujuy', 'tilcara', 3000),\n",
    "         ('santa fe', 'santa fe', 25000),\n",
    "         ('santa fe', 'colon', 31000)]\n",
    "\n",
    "rdd = sc.parallelize(censo)\n",
    "print((rdd.filter(lambda x: (20000 <= x[2] <= 50000))\\\n",
    "   .map(lambda x: (x[0], 1))\\\n",
    "   .reduceByKey(add)\\\n",
    "   .reduce(lambda x, y: x if (x[1] > y[1]) else y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bs.as', 'cf', 20000), ('santa fe', 'rosario', 30000)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.takeSample(True,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdd.takeSample(True,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((13, 'alfa'), 60)\n"
     ]
    }
   ],
   "source": [
    "#Ejercicio 2 del recuperatorio\n",
    "# 1 parte\n",
    "grados = [('alfa', 2011, 12, 12, 12, 33, 20), \n",
    "          ('beta', 2011, 2, 30, 14, 52, 18),\n",
    "          ('alfa', 2014, 12, 12, 15, 12, 23),\n",
    "          ('alfa', 2014, 12, 12, 5, 12, 10), \n",
    "          ('beta', 2001, 2, 31, 14, 52, -3),\n",
    "          ('beta', 2001, 2, 29, 14, 52, -10),\n",
    "          ('alfa', 2013, 13, 13, 23, 7, -40)]\n",
    "\n",
    "#resultado (('alfa', 123), 21.5)\n",
    "from operator import add\n",
    "\n",
    "rdd = sc.parallelize(grados)\n",
    "rdd2 = sc.parallelize([])\n",
    "\n",
    "rdd = rdd.map(lambda x: ((x[0], x[1], x[2], x[3]), (x[6], 1)))\\\n",
    "         .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "         .map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
    "        \n",
    "rdd2 = rdd\n",
    "\n",
    "rdd = rdd.map(lambda x:((x[0][3] + 1, x[0][0]), x[1]))\n",
    "rdd2 = rdd2.map(lambda x: ((x[0][3], x[0][0]), x[1]))\n",
    "print(rdd.join(rdd2)\\\n",
    "   .map(lambda x: (x[0], abs(x[1][1] - x[1][0])))\\\n",
    "   .reduce(lambda x, y: x if (x[1] > y[1]) else y))\n",
    "   \n",
    "\n",
    "#   .reduceByKey(lambda x, y: (abs(y[1] - x[1]))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 3, 1, 1)), ('b', (0, 2))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", (1,3)), (\"b\", (0,2)), (\"a\", (1,1))])\n",
    "rdd.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 9.0 failed 1 times, most recent failure: Lost task 2.0 in stage 9.0 (TID 46, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nTypeError: <lambda>() takes exactly 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nTypeError: <lambda>() takes exactly 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1db07da1c232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \"\"\"\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 9.0 failed 1 times, most recent failure: Lost task 2.0 in stage 9.0 (TID 46, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nTypeError: <lambda>() takes exactly 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/alex92/Documentos/Datos/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\nTypeError: <lambda>() takes exactly 2 arguments (1 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#Tenemos información sobre las compras que los usuarios realizan en un\n",
    "#sitio online, cada registro indica el usuario, el id de la operación y \n",
    "#una lista de productos comprados, esta información se encuentra en un RDD\n",
    "#llamado \"purchases\" (operation_id, (user_id, [product_ids])).\n",
    "#Queremos construir una lista de productos comprados frecuentemente junto\n",
    "#con otros, para eso queremos construir un nuevo RDD de la forma \n",
    "#(product_id,[p_id1,p_id2,p_id3]) donde p_id1,p_id2 y p_id3 son los tres \n",
    "#productos que mas frecuentemente se compran junto con el product_id.\n",
    "\n",
    "\n",
    "def func2(lista1, lista2):\n",
    "    \n",
    "    lista_r = []\n",
    "    \n",
    "    for e in lista1:\n",
    "        \n",
    "        if e in lista2:\n",
    "            \n",
    "            lista_r.append(e)\n",
    "            \n",
    "            for i in lista2:\n",
    "                \n",
    "                if i != e:\n",
    "                    \n",
    "                    lista_r.append(i)\n",
    "    return lista_r\n",
    "    \n",
    "\n",
    "\n",
    "#def funcion(lista2,e)\n",
    "#lista_r = []\n",
    "#  if e in lista2:\n",
    "#    if i != e:\n",
    "#      lista_r.append(i)\n",
    "# return (e,lista_r)\n",
    "\n",
    "#lista_r = []\n",
    "#for e in lista1:\n",
    "#  if e in lista2:\n",
    "#   lista_r.append(e)\n",
    "#   for i in lista1\n",
    "#    if i != e:\n",
    "#      lista_r.append(i)\n",
    "# return lista_r\n",
    "#########################################################################\n",
    "\n",
    "compras = [(23, (2, [1,2,3])),\n",
    "           (3, (22, [3,5,1])),\n",
    "           (33, (1, [34])),\n",
    "           (9, (7, [4,5,6])),\n",
    "           (13, (8, [1,3,5])),\n",
    "           (123, (9, [6,7,8])),\n",
    "           (100, (30, [1,2,3,4,5,6,7,8,9]))]\n",
    "\n",
    "rdd = sc.parallelize(compras)\n",
    "rdd.map(lambda x: (x[1][1]))\\\n",
    "   .map(lambda x, y: func2(x,y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(id_batalla, (nombre_medabot, [lista_de_medabots_vencidos]))\n",
    "#quiero una lista de mayor a menor de los nombres de los 5 medabots \n",
    "#que mas veces fueron derrotados\n",
    "\n",
    "medabatallas = [(23, ('Robo-Emperador', [123,212,423,456])),\n",
    "                (22, ('brass', [456, 423])), \n",
    "                (212, ('metabee', [283, 973, 989, 456])),\n",
    "                (100, ('Mega-Emperador', [283, 423])),\n",
    "                (34, ('art-bettle', [989, 456]))]\n",
    "rdd2 = sc.parallelize(medabatallas)\n",
    "\n",
    "medabots = [('rokusho', 123, 'kbt', 90),\n",
    "            ('art-bettle', 789, 'kbt',95),\n",
    "            ('sumilidon', 283, 'smd', 88),\n",
    "            ('brass', 212, 'brs', 40),\n",
    "            ('worbandi', 788, 'war', 98),\n",
    "            ('metabee', 423,'kbt', 90),\n",
    "            ('Robo-Emperador', 973, 'gnt', 101),\n",
    "            ('Mega-Emperador', 989, 'gnt', 110),\n",
    "            ('totalaiser', 456, 'tot', 63)]\n",
    "\n",
    "rdd = sc.parallelize(medabots)\n",
    "\n",
    "from operator import add\n",
    "\n",
    "rdd = rdd.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "rdd2.flatMap(lambda x:x[1][1])\\\n",
    "#    .map(lambda x: (x,1))\\\n",
    "#    .reduceByKey(add)\\\n",
    "#    .join(rdd)\\\n",
    "#    .map(lambda x: (x[1][1], x[1][0]))\\\n",
    "#    .takeOrdered(3, key = lambda x: -x[1])#esta linea reemplaza a las 2 de abajo\n",
    "#    .sortBy(lambda x: -x[1])\\\n",
    "#    .take(3)\n",
    "    \n",
    "\n",
    "\n",
    "#variante: añadir el factor empate ej: metabee y worbandi\n",
    "#añadir una lista para cada medabots, junto con los 3 primeros que mas veces\n",
    "#aparece en una lista,como el ej de arriba\n",
    "#otra jugar con el modelo kbt etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medabots = [('rokusho', 123, 'kbt', 90),\n",
    "            ('art-bettle', 789, 'kbt',95),\n",
    "            ('sumilidon', 283, 'smd', 88),\n",
    "            ('brass', 212, 'brs', 40),\n",
    "            ('worbandi', 788, 'war', 98),\n",
    "            ('metabee', 423,'kbt', 90),\n",
    "            ('Robo-Emperador', 973, 'gnt', 101),\n",
    "            ('Mega-Emperador', 989, 'gnt', 110),\n",
    "            ('totalaiser', 456, 'tot', 63)]\n",
    "\n",
    "rdd = sc.parallelize(medabots)\n",
    "\n",
    "\n",
    "   .     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Un telescopio registra automaticamente la luminosidad de distintas\n",
    "#estrellas generando un RDD con registros de tipo\n",
    "#(star_id, magnitude,spectral_type, timestamp). Queremos obtener un listado\n",
    "#de estrellas que tienen el mismo tipo espectral e igual promedio de \n",
    "#magnitud una vez redondeado el mismo a un decimal. El resultado debe ser \n",
    "#una lista en donde cada elemento de la lista es una lista de ids de \n",
    "#estrellas similares.\n",
    "\n",
    "stars = [\n",
    "    (1, 5, 1, 1),\n",
    "    (2, 10, 1, 1),\n",
    "    (3, 6, 1, 1),\n",
    "    (4, 5.5, 2, 1),\n",
    "    (1, 6, 1, 2),\n",
    "    (2, 9, 1, 2),\n",
    "    (3, 5, 1, 2),\n",
    "    (1, 5.5, 1, 3),\n",
    "    (2, 11, 1, 3),\n",
    "    (3, 5.5, 1, 3)\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(stars)\n",
    "\n",
    "rdd.map(lambda x: (x[0], (x[2], x[1], 1)))\\\n",
    "   .reduceByKey(lambda x, y: (x[0], x[1] + y[1], x[2] + y[2]))\\\n",
    "   .map(lambda x: ((x[1][0], x[1][1] / x[1][2]), x[0]))\\\n",
    "   .groupByKey().mapValues(lambda x: list(x))\\\n",
    "   .map(lambda x: (list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#En este ejercicio queremos programar un sistema que recomiende\n",
    "#textos a usuarios en base a sus gustos sobre ciertos términos (palabras).\n",
    "#Se cuenta con un RDD de textos de la forma (docId, texto) donde texto\n",
    "#es un string de longitud variable. Además contamos con un RDD que\n",
    "#indica qué términos le gustan o no a cada usuario de la forma (userId,\n",
    "#término, score) por ejemplo (23, “calesita”, -2). Se pide programar en\n",
    "#Spark un programa que calcule el score total de cada documento para\n",
    "#cada usuario generando un RDD de la forma (userId, docId, score) en\n",
    "#donde el score es simplemente la suma de los scores del usuario para\n",
    "#los términos que aparecen en el documento. Puede haber términos en\n",
    "#los documentos para los cuales no exista score de algunos usuarios, en\n",
    "#estos casos simplemente los consideramos neutros (score=0).  \n",
    "\n",
    "def func(unaTupla):\n",
    "    \n",
    "    print(unaTupla)\n",
    "    lista = []\n",
    "    \n",
    "    for t in unaTupla[0]:\n",
    "        \n",
    "        lista.append((t, unaTupla[1]))\n",
    "        \n",
    "     \n",
    "    return lista \n",
    "    \n",
    "\n",
    "####################################################\n",
    "\n",
    "textos = [(3, 'futbol en los mundiales'),\n",
    "          (6, 'cocinar y limpiar'),\n",
    "          (9, 'economia y politica'),\n",
    "          (65, 'java el lenguaje del futuro')]\n",
    "\n",
    "terminos =[(23, 'cocinar', -4),\n",
    "           (24, 'futbol', 10),\n",
    "           (17, 'limpiar', 8),\n",
    "           (24, 'limpiar', -1),\n",
    "           (17, 'futbol', 1),\n",
    "           (24, 'mundiales', 9),\n",
    "           (24, 'economia', 2),\n",
    "           (24, 'politica', 3),\n",
    "           (23, 'java', 10)]\n",
    "\n",
    "# Gaston: def funcion(tu)\n",
    "#lista = []\n",
    "#for t in tu[0]:\n",
    "#  lista.append((t,tu[1]))\n",
    "#return lista                        \n",
    "#Gaston: donde tu es la tupla tuya\n",
    "from operator import add\n",
    "\n",
    "rdd1 = sc.parallelize(textos)\n",
    "rdd2 = sc.parallelize(terminos)\n",
    "\n",
    "rdd1 = rdd1.map(lambda x: ((x[1].split(' ')), x[0]))\\\n",
    "           .map(lambda x: func(x))\\\n",
    "           .flatMap(lambda x: x)\n",
    "        \n",
    "rdd2.map(lambda x: (x[1], (x[0], x[2])))\\\n",
    "    .join(rdd1)\\\n",
    "    .map(lambda x: ((x[1][0][0], x[1][1]), x[1][0][1]))\\\n",
    "    .reduceByKey(add).collect()\n",
    "\n",
    "#[(('futbol', 'en', 'los', 'mundiales'), 3)]\n",
    "#[(('futbol', 3), ('en', 3), ('los', 3), ('mundiales', 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), ('c',3)])\n",
    "y = sc.parallelize([(\"a\", ('12', 4)), (\"a\", ('papa', '333')), ('c',5), ('a', 10)])\n",
    "print(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[48] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "censo = [('bs.as', 'cf', 20000), \n",
    "         ('santa fe', 'rosario', 30000),\n",
    "         ('bs.as', 'mar del plata', 10000),\n",
    "         ('bs.as', 'la matanza', 40000),\n",
    "         ('jujuy', 'tilcara', 3000),\n",
    "         ('santa fe', 'santa fe', 25000),\n",
    "         ('santa fe', 'colon', 31000)]\n",
    "\n",
    "rdd = sc.parallelize(censo)\n",
    "\n",
    "rdd.map(lambda x: (x[0], x[2]))\\\n",
    "   .reduceByKey(lambda x, y: x if (x > y) else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
